{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VccPFI-Sp57X"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlD96dTNnE-6"
      },
      "source": [
        "# Assignment 1: Build a parser and tokenizer for Wikipedia content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD1VrZ0Meu0r"
      },
      "source": [
        "## General guidelines\n",
        "\n",
        "This notebook contains considerable amount of code to help you complete this assignment. Your task is to implement any missing parts of the code and answer any questions (if exist) within this notebook. This will require understanding the existing code, may require reading about packages being used, reading additional resources, and maybe even going over your notes from class ðŸ˜±\n",
        "\n",
        "**Evaluation and auto-grading**: Your submissions will be evaluated using both automatic and manual grading. Code parts for implementation are marked with a comment `# YOUR CODE HERE`, and usually followed by cell(s) containing automatic tests that evaluate the correctness of your answer. Staff will allow your notebook to **execute from start to finish for no more than 90 seconds**, then manually assess your submission. Any automatic tests that did not run due to your notebook timing out **will automatically receive 0 points**. The execution time excludes initial data download, which will already exist in the testing environment. The staff reserves the right to **modify any grade provided by the auto-grader** as well as to **execute additional tests not provided to you**. It is also important to note that **auto-graded cells only result in full or no credit**. In other words, you must pass all tests implemented in a test cell in order to get the credit for it, and passing some, but not all, of the tests in a test cell will not earn you any points for that cell. \n",
        "\n",
        "**Submission**: Unless specified otherwise, you need to upload this notebook file **with your ID as the file name**, e.g. 012345678.ipynb, to the assignment on Moodle. Before submitting, **make sure the notebook executes from start to finish in less than 90 seconds**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VccPFI-Sp57X"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "In this assignment, we are going to read, parse, and tokenize a small number of Wikipedia articles from a single part file, and learn how to collect and merge page view information with articles. By the end of this assignment, you will be able to:\n",
        "\n",
        "1. (25 Points) Parse and clean Wikipedia page entries from one part file of the wiki dump. From each page, extract the title, body, and a list of (anchor text, page title) pairs for links to other English Wikipedia pages. \n",
        "2. (70 Points) Tokenize the text of Wikipedia articles using a regex tokenizer. \n",
        "3. (5 Points) Collect page view information from Wikipedia and merge that with articles' data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAdNnF04t3Hd"
      },
      "source": [
        "import numpy as np\n",
        "import bz2\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from itertools import islice\n",
        "from xml.etree import ElementTree\n",
        "import codecs\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path"
      ],
      "execution_count": 493,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuDnZdXSsDJj"
      },
      "source": [
        "# 1. Parsing a dump file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4_pHk6vyc1G"
      },
      "source": [
        "First, let's download the file and examine it a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Lx1Y81O5uX_o",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "63ebf151c477e7b9209c91a26f0f892c",
          "grade": false,
          "grade_id": "data-download",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10855ec-efca-4b19-a058-9837dc339868"
      },
      "source": [
        "## Download one wikipedia file\n",
        "part_url = 'https://dumps.wikimedia.org/enwiki/20210801/enwiki-20210801-pages-articles-multistream15.xml-p17324603p17460152.bz2'\n",
        "wiki_file = Path(part_url).name\n",
        "!wget -N $part_url"
      ],
      "execution_count": 494,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-06 14:55:32--  https://dumps.wikimedia.org/enwiki/20210801/enwiki-20210801-pages-articles-multistream15.xml-p17324603p17460152.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File â€˜enwiki-20210801-pages-articles-multistream15.xml-p17324603p17460152.bz2â€™ not modified on server. Omitting download.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdlUgKJTujCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660c70ff-d5ea-45db-bb6d-a048190dcc7b"
      },
      "source": [
        "# Make sure you downloaded the file\n",
        "!ls"
      ],
      "execution_count": 495,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enwiki-20210801-pages-articles-multistream15.xml-p17324603p17460152.bz2\n",
            "pageviews-202108-user-4dedup.txt\n",
            "pageviews-202108-user.bz2\n",
            "pageviews-202108-user.pkl\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii20yBfBxIF-"
      },
      "source": [
        "# Uncomment this to view the first 59 lines of the (uncompressed) file\n",
        "#!bzcat $wiki_file | head -n 59"
      ],
      "execution_count": 496,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABRNY_pByr8t"
      },
      "source": [
        "## Parsing from XML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3EIy8ADs2Z"
      },
      "source": [
        "The following code reads the wiki dump file, parses its XML, and iterate over pages. First, it filters out pages that are redirects to other pages, talk pages, and any other pages that are not articles pages. Then, it extracts the article id and article markup text.\n",
        "\n",
        "**YOUR TASK (5 Points)**: Implement the extraction of article title from the XML. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "661_cqDiKdHy",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "18263a8a30a0c43531ec90c46b754e2b",
          "grade": false,
          "grade_id": "page_iter",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def page_iter(wiki_file):\n",
        "  \"\"\" Reads a wiki dump file and create a generator that yields pages. \n",
        "  Parameters:\n",
        "  -----------\n",
        "  wiki_file: str\n",
        "    A path to wiki dump file.\n",
        "  Returns:\n",
        "  --------\n",
        "  tuple\n",
        "    containing three elements: article id, title, and body. \n",
        "  \"\"\"\n",
        "  # open compressed bz2 dump file\n",
        "  with bz2.open(wiki_file, 'rt', encoding='utf-8', errors='ignore') as f_in:\n",
        "    # Create iterator for xml that yields output when tag closes\n",
        "    elems = (elem for _, elem in ElementTree.iterparse(f_in, events=(\"end\",)))\n",
        "    # Consume the first element and extract the xml namespace from it. \n",
        "    # Although the raw xml has the  short tag names without namespace, i.e. it \n",
        "    # has <page> tags and not <http://wwww.mediawiki.org/xml/export...:page> \n",
        "    # tags, the parser reads it *with* the namespace. Therefore, it needs the \n",
        "    # namespace when looking for child elements in the find function as below.\n",
        "    elem = next(elems)\n",
        "    m = re.match(\"^{(http://www\\.mediawiki\\.org/xml/export-.*?)}\", elem.tag)\n",
        "    if m is None:\n",
        "        raise ValueError(\"Malformed MediaWiki dump\")\n",
        "    ns = {\"ns\": m.group(1)}\n",
        "    page_tag = ElementTree.QName(ns['ns'], 'page').text\n",
        "    # iterate over elements\n",
        "    for elem in elems:\n",
        "      if elem.tag == page_tag:\n",
        "        # Filter out redirect and non-article pages\n",
        "        if elem.find('./ns:redirect', ns) is not None or \\\n",
        "           elem.find('./ns:ns', ns).text != '0':\n",
        "          elem.clear()\n",
        "          continue\n",
        "        # Extract the article wiki id\n",
        "        wiki_id = elem.find('./ns:id', ns).text\n",
        "        # Extract the article title into a variables called title\n",
        "        # YOUR CODE HERE\n",
        "        title = elem.find('./ns:title', ns).text\n",
        "        # extract body\n",
        "        body = elem.find('./ns:revision/ns:text', ns).text\n",
        "\n",
        "        yield wiki_id, title, body\n",
        "        elem.clear()"
      ],
      "execution_count": 497,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXuZ93MVXTpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b05755e-fe48-4b36-b4ba-b73fefff22c9"
      },
      "source": [
        "# Print the first page\n",
        "p1 = next(page_iter(wiki_file))\n",
        "print(f\"{p1[0]} {p1[1]}\\n\\n{' '*80}\\n{p1[2]}\")"
      ],
      "execution_count": 498,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17324616 Langnes\n",
            "\n",
            "                                                                                \n",
            "'''Langnes''' may refer to:\n",
            "{{TOC right}}\n",
            "\n",
            "==Places==\n",
            "\n",
            "===Antarctica===\n",
            "*[[Langnes Fjord]], a fjord in Antarctica\n",
            "*[[Langnes Peninsula]], a peninsula in Antarctica\n",
            "\n",
            "===Norway===\n",
            "*[[Langnes, Troms]], a village in Lenvik municipality, Troms county, Norway\n",
            "*[[Langnes, Ã˜stfold]], a village in Askim municipality, Ã˜stfold county, Norway\n",
            "*Langnes Airport in the city of TromsÃ¸, also known as [[TromsÃ¸ Airport]]\n",
            "*[[Langnes Station]], a railway station located at Langnes in Askim municipality on the Ã˜stfold Line\n",
            "\n",
            "==People==\n",
            "*[[Ole Arvid Langnes]], a retired Norwegian football goalkeeper\n",
            "\n",
            "==Other==\n",
            "*[[Battle of Langnes]], a battle fought between Norway and Sweden as a part of the Swedish-Norwegian War of 1814\n",
            "\n",
            "==See also==\n",
            "*[[Langness]]\n",
            "*[[Langenes (disambiguation)]]\n",
            "*[[Langeness]]\n",
            "\n",
            "{{geodis}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ozXrWoJDysV5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "426cfe410e1f0a2b6e07be30cf819b0e",
          "grade": true,
          "grade_id": "extract_title",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Check the title of the first article\n",
        "assert p1[1] == 'Langnes'"
      ],
      "execution_count": 499,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uaubgcMYClb"
      },
      "source": [
        "## Parsing articles (MediaWiki)\n",
        "\n",
        "Wikipedia articles are written in a special markdown format called [MediaWiki](https://en.wikipedia.org/wiki/MediaWiki). The format is described [here](https://www.mediawiki.org/wiki/Help:Formatting) and you will need to read a little bit about it in order to complete this assignment successfully. A key property of the MediaWiki markdown is that it's recursive -- markdown can be (and often does) nest inside other markdown elements. For example, a wiki link can contain another wiki link:\n",
        "```\n",
        "[[File:image1.jpg|[[Wikipedia]]]]\n",
        "```\n",
        "\n",
        "Fortunately, there are implementations of parsers for MediaWiki, and we are going to use one called `mwparserfromhell`. Let's import/install it first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o0r_AHP2GOJ"
      },
      "source": [
        "try:\n",
        "    import mwparserfromhell as mwp\n",
        "except ImportError:\n",
        "    !pip install -I mwparserfromhell==0.6.0\n",
        "finally:\n",
        "    import mwparserfromhell as mwp\n",
        "# modify the parser behavior a bit, no need to understand this code.\n",
        "mwp.definitions.INVISIBLE_TAGS.append('ref')"
      ],
      "execution_count": 500,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmUhEFFf8Ww8"
      },
      "source": [
        "Let's parse some MediaWiki and look at the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3P9QXKG84QB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948f46c6-5526-4c7d-8b01-477001b5821d"
      },
      "source": [
        "wikicode = mwp.parse('Lorem ipsum {{foo|bar|{{baz}}|spam=eggs}}')\n",
        "print(wikicode.get_tree())"
      ],
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lorem ipsum \n",
            "{{\n",
            "      foo\n",
            "    | 1\n",
            "    = bar\n",
            "    | 2\n",
            "    = {{\n",
            "            baz\n",
            "      }}\n",
            "    | spam\n",
            "    = eggs\n",
            "}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ytu9ka95k89"
      },
      "source": [
        ""
      ],
      "execution_count": 501,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0nW0PtC5kg4"
      },
      "source": [
        ""
      ],
      "execution_count": 501,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1Ds3wZ2-eM-"
      },
      "source": [
        "Naturally, the result of parsing the MediaWiki format results is **a tree**. In the above case, the markdown contains a [template](https://www.mediawiki.org/wiki/Help:Templates) named foo, which takes two positional parameters, another template called baz, and a named parameter called spam. Templates are both cool (in providing structured information) and wild (in the sense that their structure keeps changing). \n",
        "\n",
        "A particular type of template called **Infobox** is the backbone of some of the largest knowledge bases on the planet(!), powering DBPedia and many popular IR applications such as Apple's Siri, Google Search, and more. To get a sense of the richness of infoboxes, let's look at one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiaDfnCxEVw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd72ebe6-4c02-4265-c30d-18c0e3df438b"
      },
      "source": [
        "iter = page_iter(wiki_file)\n",
        "next(iter)\n",
        "p2 = next(iter)\n",
        "wikicode = mwp.parse(p2[2], skip_style_tags=True)\n",
        "for template in wikicode.ifilter_templates():\n",
        "  if str(template.name).strip().startswith('Infobox'):\n",
        "    print(template)"
      ],
      "execution_count": 502,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{{Infobox settlement\n",
            "|official_name          =Langenes\n",
            "|other_name             =\n",
            "|native_name            =  \n",
            "|nickname               = \n",
            "|image_skyline=\n",
            "|image_caption = \n",
            "|settlement_type        = Village\n",
            "|pushpin_map            = Norway\n",
            "|pushpin_label_position = bottom\n",
            "|pushpin_mapsize        =\n",
            "|pushpin_map_caption    = Location in Norway\n",
            "|subdivision_type       = Country\n",
            "|subdivision_name       = [[Norway]]\n",
            "|subdivision_type1      = [[List of regions of Norway|Region]] \n",
            "|subdivision_name1      = [[Northern Norway]]\n",
            "|subdivision_type2      = [[Counties of Norway|County]] \n",
            "|subdivision_name2      = [[Troms og Finnmark]]\n",
            "|subdivision_type3      = [[List of municipalities of Norway|Municipality]]\n",
            "|subdivision_name3      =\n",
            "|area_footnotes         = \n",
            "|area_total_km2         = \n",
            "|population_as_of       = \n",
            "|population_footnotes   = \n",
            "|population_total       =\n",
            "|timezone1              = [[Central European Time|CET]] \n",
            "|utc_offset1            = +01\n",
            "|timezone1_DST          = [[Central European Summer Time|CEST]] \n",
            "|utc_offset1_DST        = +02\n",
            "|postal_code_type       = \n",
            "|postal_code            =\n",
            "|coordinates            = {{coord|70|6|N|23|0|E|type:city|display=inline}}\n",
            "}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu6etF_YiSwo"
      },
      "source": [
        "Look at all this beautifully-structured data about a random village in Norway!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFWKVm3XPLAJ"
      },
      "source": [
        "## Parsing outgoing article links (wiki links)\n",
        "\n",
        "Putting aside templates and infoboxes, one of the key elements we care about in search engines is links. The text of the link, also known as anchor text, is a very useful description of the page that is being linked. Analyzing the link structure between pages using algorithms like PageRank, which we will cover later in class, is one of the key factors for improving search results. \n",
        "\n",
        "In this assignment, we're going to focus on links from one wikipedia article to another wikipedia article.\n",
        "\n",
        "**YOUR TASK (20 POINTS)**: Complete the impementation of `get_wikilinks` and `filter_article_links` to return a list of (link, anchor text) for all outgoing links from an article to other wikipedia articles that it points to. See MediaWiki's format for [internal links](https://www.mediawiki.org/wiki/Help:Links#Internal_links). Please remove from the link any reference to a section/anchor in the target page. You are free to implement your filter through a regex or some other means. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "ScR7lH6TYf9k",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8299df874b6c7128131c500a17afc7c5",
          "grade": false,
          "grade_id": "links",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def filter_article_links(title):\n",
        "  \"\"\" Return false for wikilink titles (str) pointing to non-articles such as images, files, media and more (as described in the documentation).\n",
        "      Otherwise, returns true. \n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  x = re.search(\"Special:|Help:|skype:|File:|Category:|User:|Manual:|{{TALKPAGENAME}}|Extension:|mailto:|:|https://\", title)\n",
        "  if x:\n",
        "    return False\n",
        "\n",
        "  return True\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "def get_wikilinks(wikicode):\n",
        "  \"\"\" Traverses the parse tree for internal links and filter out non-article \n",
        "  links.\n",
        "  Parameters:\n",
        "  -----------\n",
        "  wikicode: mwp.wikicode.Wikicode\n",
        "    Parse tree of some WikiMedia markdown.\n",
        "  Returns:\n",
        "  --------\n",
        "  list of (link: str, anchor_text: str) pair\n",
        "    A list of outgoing links from the markdown to wikipedia articles.\n",
        "  \"\"\"\n",
        "  links = []\n",
        "  for wl in wikicode.ifilter_wikilinks():\n",
        "    # skip links that don't pass our filter\n",
        "    title = str(wl.title)\n",
        "    if not filter_article_links(title):\n",
        "      continue\n",
        "    # if text is None use title, otherwise strip markdown from the anchor text.\n",
        "    text = wl.text\n",
        "    if text is None:\n",
        "      text = title\n",
        "    else:\n",
        "      text = text.strip_code()\n",
        "    # remove any lingering section/anchor reference in the link\n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    if '#' in title:\n",
        "      if title[0]=='#':\n",
        "        continue\n",
        "      else:\n",
        "        index = title.index('#')\n",
        "        title = title[:index]   \n",
        "      \n",
        "        \n",
        "          \n",
        "\n",
        "   \n",
        "    \n",
        "    links.append((title, text))\n",
        "  return links"
      ],
      "execution_count": 503,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PCiW-Hq0rJet",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "276c9a282515f0e424ae468b6f37546d",
          "grade": true,
          "grade_id": "links-basic",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122937ac-0049-40c1-a370-f79a2dfc701b"
      },
      "source": [
        "# Basic checks that we can extract links correctly\n",
        "get_wl = lambda text: get_wikilinks(mwp.parse(text))\n",
        "assert get_wl(\"[[Wikipedia]]\")[0] == ('Wikipedia', 'Wikipedia')\n",
        "assert get_wl(\"[[Wikipedia|some text]]\")[0] == ('Wikipedia', 'some text')\n",
        "\n",
        "assert len(get_wl(\"[[File:example.jpg|frame|caption]]\")) == 0\n",
        "print(get_wl('[[Wikipedia#Preview|preview]]'))\n",
        "assert get_wl('[[Wikipedia#Preview|preview]]')[0] == ('Wikipedia', 'preview')"
      ],
      "execution_count": 504,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Wikipedia', 'preview')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "C80Zs6L724OG",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c4ff4653b5984f1993ac64be339a15a0",
          "grade": true,
          "grade_id": "links-adv",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# More advanced tests\n",
        "assert len(get_wl(p2[2])) < len(mwp.parse(p2[2]).filter_wikilinks())\n",
        "pages = list(islice(page_iter(wiki_file), None, 25))\n",
        "p4, p14, p24 = pages[4], pages[14], pages[24]\n",
        "assert len(get_wl(p4[2])) == 32\n",
        "assert len(get_wl(p14[2])) == 891\n",
        "assert len(get_wl(p24[2])) == 25"
      ],
      "execution_count": 505,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHZo7qtJDCI_"
      },
      "source": [
        "# 2. Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl2o0OtM3sMn"
      },
      "source": [
        "Before tokenizing Wikipedia articles' text we need to remove any remaining MediaWiki markdown from the text. Luckily, our parser knows how to strip all markdown as demonstrated by the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Mm27-Xmjvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c2f035-545a-47c0-97a7-453d018676e2"
      },
      "source": [
        "def remove_markdown(text):\n",
        "  return mwp.parse(text).strip_code()\n",
        "print(remove_markdown(\"\"\"\n",
        "== Section 2 ==\n",
        "[[File:image1.jpg| '''''beautiful''''' <b>image</b> of [[Wikipedia]]]]\n",
        "\"\"\"))"
      ],
      "execution_count": 506,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Section 2 \n",
            " beautiful image of Wikipedia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW_dZl9pG5Mz"
      },
      "source": [
        "Great! now we can focus on tokenzing the clean text. Here's the clean text of one article after preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZa97L8KsrMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ebbd6f-4e43-4220-8ee0-98336ce58bfd"
      },
      "source": [
        "print(remove_markdown(p4[2]))"
      ],
      "execution_count": 507,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Houstonia longifolia, commonly known as long-leaved bluet or longleaf summer bluet, is a perennial plant in the family Rubiaceae. It can be found throughout most of the Eastern United States and Canada. It has been reported from every state east of the Mississippi River except Delaware, plus North Dakota, Minnesota, Missouri, Arkansas and Oklahoma, with isolated populations in Kansas and Texas. Also, all Canadian provinces from Quebec to Alberta. It prefers upland woods in poor, dry, often sandy soil, blooming from June to August.\n",
            "\n",
            "Varieties\n",
            "Two varieties are recognized:\n",
            "\n",
            "Houstonia longifolia var. longifolia - From Georgia and Arkansas north to Canada\n",
            "Houstonia longifolia var. tenuifolia (Nutt.) Alph.Wood. - Florida, Georgia, Tennessee, Virginia\n",
            "thumb|left\n",
            "\n",
            "References\n",
            "\n",
            "External links\n",
            " USDA PLANTS Profile\n",
            "Photo of herbarium specimen at Missouri Botanical Garden, isotype of Houstonia longifolia\n",
            "\n",
            "Longifolia\n",
            "Category:Flora of the Eastern United States\n",
            "Category:Flora of the United States\n",
            "Category:Flora of Alberta\n",
            "Category:Flora of Manitoba\n",
            "Category:Flora of Quebec\n",
            "Category:Flora of Ontario\n",
            "Category:Flora of Saskatchewan\n",
            "Category:Plants described in 1788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHZpZM6qT3MK"
      },
      "source": [
        "**YOUR TASK (70 POINTS)**: Complete the implementation of the functions in the next cell that return regular expressions (as strings) to capture dates, time, etc. in the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "PoQ6TG0buRAL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "64d35f0cee87a117caeb4f1516c59a62",
          "grade": false,
          "grade_id": "tok-ans",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def get_html_pattern():\n",
        "  return \"\"\"(<[^/>][^>]*>|</[^>]+>)\"\"\"\n",
        "def get_date_pattern():\n",
        "  \n",
        "  return \"\"\"((Sep(tember)?|Apr(il)?|Jun(e)?|Nov(ember)?)+\\s(0?[1-9]|[12]\\d|3[0]),\\s+\\d{4})|((Jan(uary)?|Mar(ch)?|May|Jul(y)?|Aug(ust)?|Oct(ober)?|Dec(ember)?)+\\s(0?[1-9]|[12]\\d|3[01]),\\s+\\d{4})|\n",
        "((0?[1-9]|[12]\\d|3[0])\\s+(Apr(il)?|Jun(e)?|Sep(tember)?|Nov(ember)?)\\s+\\d{4})|((0?[1-9]|[12]\\d|3[01])\\s+(Jan(uary)?|Mar(ch)?|May|Jul(y)?|Aug(ust)?|Oct(ober)?|Dec(ember)?)\\s+\\d{4})|\n",
        "((Feb(ruary)?)+\\s(0?[1-9]|[12]\\d),\\s+\\d{4})|((0?[1-9]|[12]\\d)\\s+(Feb(ruary)?)\\s+\\d{4})\"\"\"\n",
        "       \n",
        "\n",
        " \n",
        "  #return \"\"\"((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+\\d{1,2},\\s+\\d{4})\"\"\"\n",
        " # return \"\"\"((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+\\d{1,2},\\s+\\d{4})|\n",
        "#(\\d{1,2}\\s+(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+\\d{4})\"\"\"\n",
        " \n",
        "\n",
        "def get_time_pattern():\n",
        "\n",
        "  return \"\"\"((1[0-2]|[0-9])([0-5][0-9])((a|A|p|P)(\\.)?(m|M)(\\.))|^(1[0-2]|[0-9])(\\.|\\:)([0-5][0-9])((a|A|p|P)(\\.)?(m|M)(\\.)?)|(1[0-2]|[0-9])(\\.|\\:)([0-5][0-9])(\\.|\\:)([0-5][0-9]$))\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_percent_pattern():\n",
        "  return  \"\"\"(?<![\\w\\+\\-,\\.])[\\+\\-]?\\d{1,3}((,\\d{3})|\\d)(\\.\\d+)?%(?!\\S?[\\w\\+\\-])\"\"\"\n",
        "  \n",
        "def get_number_pattern():\n",
        "  \n",
        "  return \"\"\"(?<![+-\\.]|[0-9])((\\+|\\-)?(?<![a-zA-Z])(\\d{1,3})(,\\d{3})*(\\.\\d+)?(?!([0-9]|\\.[0-9a-zA-Z]|,[0-9a-zA-Z])))\"\"\"\n",
        " \n",
        "  \n",
        "  \n",
        "def get_word_pattern():\n",
        "  return \"\"\"((?<![-])+\\w([a-zA-Z'-]+)) \"\"\"\n"
      ],
      "execution_count": 508,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb08frfeglgF"
      },
      "source": [
        ""
      ],
      "execution_count": 508,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfychz9NK9HI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DqboCsWlyvl"
      },
      "source": [
        ""
      ],
      "execution_count": 508,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QroVdm-ukzlc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tvNGAvTccGHw",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "72ba652e23237f58d82216aba055fc15",
          "grade": false,
          "grade_id": "cell-218dd071f62181a3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "RE_TOKENIZE = re.compile(rf\"\"\"\n",
        "(\n",
        "    # parsing html tags\n",
        "     (?P<HTMLTAG>{get_html_pattern()})\n",
        "                                       \n",
        "    # dates\n",
        "    |(?P<DATE>{get_date_pattern()})\n",
        "    # time\n",
        "    |(?P<TIME>{get_time_pattern()})\n",
        "    # Percents\n",
        "    |(?P<PERCENT>{get_percent_pattern()})\n",
        "    # Numbers\n",
        "    |(?P<NUMBER>{get_number_pattern()})\n",
        "    # Words\n",
        "    |(?P<WORD>{get_word_pattern()})\n",
        "    # space\n",
        "    |(?P<SPACE>[\\s\\t\\n]+) \n",
        "    # everything else\n",
        "    |(?P<OTHER>.)\n",
        "    )\n",
        "    \"\"\",\n",
        "      re.MULTILINE | re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
        "\n",
        "def tokenize(text):\n",
        "  return [(v, k) for match in RE_TOKENIZE.finditer(text)\n",
        "                 for k, v in match.groupdict().items() \n",
        "                 if v is not None and k != 'SPACE']"
      ],
      "execution_count": 509,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "IjSOkJ9rRhhf",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "55957def7ed9332ea774d8ebcb8d0915",
          "grade": true,
          "grade_id": "html-basic",
          "locked": true,
          "points": 7,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead25d72-ba1c-4c9a-a729-5414332469f0"
      },
      "source": [
        "# html basic tests (7 points)\n",
        "def tok(text):\n",
        "  return tokenize(remove_markdown(text))\n",
        "tokens = tok(r'<nowiki><b>hello</b></nowiki>')\n",
        "print(tokens)\n",
        "assert ('<b>', 'HTMLTAG') in tokens\n",
        "assert ('</b>', 'HTMLTAG') in tokens\n",
        "tokens = tok(r'<nowiki><b style=\"color:red\">hello</b></nowiki>')\n",
        "assert ('<b style=\"color:red\">', 'HTMLTAG') in tokens\n",
        "tokens = tok(r'<nowiki><br /></nowiki>')\n",
        "assert ('<br />', 'HTMLTAG') in tokens"
      ],
      "execution_count": 510,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<b>', 'HTMLTAG'), ('hello', 'WORD'), ('</b>', 'HTMLTAG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GT8hBUEJWuGV",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7fd0bcdae93fa1e299649566d2851e1b",
          "grade": true,
          "grade_id": "html-adv",
          "locked": true,
          "points": 6,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# html advanced tests (6 points)\n",
        "tokens = tok(r'<nowiki><b><i>hello</i></b></nowiki>')\n",
        "assert 4 == sum([1 for _, t in tokens if t == 'HTMLTAG'])"
      ],
      "execution_count": 511,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "qab7jLsOX_jH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "84440df7d383a67d60384c79fb9bf1b7",
          "grade": true,
          "grade_id": "dates-basic",
          "locked": true,
          "points": 7,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972dbbfb-16d5-4615-9672-568559252258"
      },
      "source": [
        "# dates basic tests (7 points)\n",
        "tokens = tok(r'dates in the format of January 29, 1984, Nov 3, 2020, or 3 Nov 2020.')\n",
        "print(tokens)\n",
        "assert ('January 29, 1984', 'DATE') in tokens\n",
        "assert ('Nov 3, 2020', 'DATE') in tokens\n",
        "assert ('3 Nov 2020', 'DATE') in tokens"
      ],
      "execution_count": 512,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dates', 'WORD'), ('in', 'WORD'), ('the', 'WORD'), ('format', 'WORD'), ('of', 'WORD'), ('January 29, 1984', 'DATE'), (',', 'OTHER'), ('Nov 3, 2020', 'DATE'), (',', 'OTHER'), ('or', 'WORD'), ('3 Nov 2020', 'DATE'), ('.', 'OTHER')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VtVH6pAjQ_2"
      },
      "source": [
        ""
      ],
      "execution_count": 512,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1ngQ08sVaL-1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "08a2960dc053c12a3c5e5f79b23682f2",
          "grade": true,
          "grade_id": "dates-adv",
          "locked": true,
          "points": 6,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# dates advanced tests (6 points)\n",
        "tokens = tok(r'Sep 29, 1984, Apr 33, 2020, or 30 feb 2020.')\n",
        "assert ('Sep 29, 1984', 'DATE') in tokens\n",
        "assert ('Apr 33, 2020', 'DATE') not in tokens\n",
        "assert ('30 feb 2020', 'DATE') not in tokens"
      ],
      "execution_count": 513,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "iDE8L6WxD8IJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "22dc3e966cdd3ad42ea22edd9cf6c6ec",
          "grade": true,
          "grade_id": "time-basic",
          "locked": true,
          "points": 7,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e0aceb-aced-4449-c11b-64adccd201dd"
      },
      "source": [
        "# time basic tests (7 points)\n",
        "tokens = tok(r'12.12PM 1202a.m. 6:12:12')\n",
        "print(tokens)\n",
        "assert ('12.12PM', 'TIME') in tokens\n",
        "assert ('1202a.m.', 'TIME') in tokens\n",
        "assert ('6:12:12', 'TIME') in tokens"
      ],
      "execution_count": 514,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('12.12PM', 'TIME'), ('1202a.m.', 'TIME'), ('6:12:12', 'TIME')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "V7YzBe-8INrQ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b4cdfc9a1967c9afaeebcc6a25102330",
          "grade": true,
          "grade_id": "time-adv",
          "locked": true,
          "points": 6,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b089569-d497-422c-d53b-6b89a62ef9b3"
      },
      "source": [
        "# time advanced tests (6 points)\n",
        "tokens = tok(r'36.12PM 1272a.m. 1202a.m 12:12:12am 56:12:12 6:72:12')\n",
        "print(tokens)\n",
        "print(sum([1 for _, t in tokens if t == 'TIME']))\n",
        "assert 0 == sum([1 for _, t in tokens if t == 'TIME'])"
      ],
      "execution_count": 515,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('36.12', 'NUMBER'), ('PM', 'WORD'), ('1', 'OTHER'), ('2', 'OTHER'), ('7', 'OTHER'), ('2a', 'WORD'), ('.', 'OTHER'), ('m', 'OTHER'), ('.', 'OTHER'), ('1', 'OTHER'), ('2', 'OTHER'), ('0', 'OTHER'), ('2a', 'WORD'), ('.', 'OTHER'), ('m', 'OTHER'), ('12', 'NUMBER'), (':', 'OTHER'), ('12', 'NUMBER'), (':', 'OTHER'), ('12', 'NUMBER'), ('am', 'WORD'), ('56', 'NUMBER'), (':', 'OTHER'), ('12', 'NUMBER'), (':', 'OTHER'), ('12', 'NUMBER'), ('6', 'NUMBER'), (':', 'OTHER'), ('72', 'NUMBER'), (':', 'OTHER'), ('12', 'NUMBER')]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BuWJFc9Wg3bf",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3758c0adc0fa462bc77a34bba2947ab9",
          "grade": true,
          "grade_id": "num-basic",
          "locked": true,
          "points": 7,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# number basic tests (7 points)\n",
        "tokens = tok(r\"\"\"12 +12 -12.0 -12,345.5466 +12,345,678,678 0.154\"\"\")\n",
        "assert ('12', 'NUMBER') in tokens\n",
        "assert ('+12', 'NUMBER') in tokens\n",
        "assert ('-12.0', 'NUMBER') in tokens\n",
        "assert ('-12,345.5466', 'NUMBER') in tokens\n",
        "assert ('+12,345,678,678', 'NUMBER') in tokens\n",
        "assert ('0.154', 'NUMBER') in tokens"
      ],
      "execution_count": 516,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHXx43S0_KaW"
      },
      "source": [
        ""
      ],
      "execution_count": 516,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "eG33yKNhDPjY",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "773499a6e2cbd3bfe06517024f6ecd5e",
          "grade": true,
          "grade_id": "num-adv",
          "locked": true,
          "points": 6,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b56d12-3d89-4efa-c50b-42a1f1c07b8d"
      },
      "source": [
        "# number advanced tests (6 points)\n",
        "assert ('500', 'NUMBER') in tok('the pound (500 in value)...')\n",
        "assert ('500', 'NUMBER') in tok('the price is 500.')\n",
        "assert ('500', 'NUMBER') in tok('the price is 500, but it is negotiable.')\n",
        "assert ('500', 'NUMBER') in tok('the price is 500: no less!')\n",
        "assert ('500', 'NUMBER') not in tok('the price rose 500%')\n",
        "tokens = tok(r\"\"\"12.A W12 +-12 -.12.0 -12,34.5466 +12,345,6+78,678 0.15,4\"\"\")\n",
        "print(tokens)\n",
        "print(sum([1 for _, t in tokens if t == 'NUMBER']))\n",
        "assert 0 == sum([1 for _, t in tokens if t == 'NUMBER'])\n"
      ],
      "execution_count": 517,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', 'OTHER'), ('2', 'OTHER'), ('.', 'OTHER'), ('A', 'OTHER'), ('W', 'OTHER'), ('1', 'OTHER'), ('2', 'OTHER'), ('+', 'OTHER'), ('-', 'OTHER'), ('1', 'OTHER'), ('2', 'OTHER'), ('-', 'OTHER'), ('.', 'OTHER'), ('1', 'OTHER'), ('2', 'OTHER'), ('.', 'OTHER'), ('0', 'OTHER'), ('-', 'OTHER'), ('1', 'OTHER'), ('2', 'OTHER'), (',', 'OTHER'), ('3', 'OTHER'), ('4', 'OTHER'), ('.', 'OTHER'), ('5', 'OTHER'), ('4', 'OTHER'), ('6', 'OTHER'), ('6', 'OTHER'), ('+', 'OTHER'), ('1', 'OTHER'), ('2', 'OTHER'), (',', 'OTHER'), ('3', 'OTHER'), ('4', 'OTHER'), ('5', 'OTHER'), (',', 'OTHER'), ('6', 'OTHER'), ('+', 'OTHER'), ('7', 'OTHER'), ('8', 'OTHER'), (',', 'OTHER'), ('6', 'OTHER'), ('7', 'OTHER'), ('8', 'OTHER'), ('0', 'OTHER'), ('.', 'OTHER'), ('1', 'OTHER'), ('5', 'OTHER'), (',', 'OTHER'), ('4', 'OTHER')]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "B_m0bKwoPuZU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ff76ea4b448fed3e9ad28c119f898ad0",
          "grade": true,
          "grade_id": "words",
          "locked": true,
          "points": 13,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54ea4b5-8ca7-4e9d-b392-adb71716b1e8"
      },
      "source": [
        "# word tests (13 points)\n",
        "tokens = tok(r\"\"\"Hello Bob! It's Mary, your mother-in-law, \n",
        "  the mistake is your parents'! --Mom\"\"\")\n",
        "print(tokens)\n",
        "assert ('Hello', 'WORD') in tokens\n",
        "assert ('Bob', 'WORD') in tokens\n",
        "assert (\"It's\", 'WORD') in tokens\n",
        "assert ('Mary', 'WORD') in tokens\n",
        "assert ('your', 'WORD') in tokens\n",
        "assert ('mother-in-law', 'WORD') in tokens\n",
        "assert (\"parents'\", 'WORD') in tokens\n",
        "assert (\"Mom\", 'WORD') not in tokens\n",
        "assert (\"-Mom\", 'WORD') not in tokens\n",
        "assert (\"--Mom\", 'WORD') not in tokens"
      ],
      "execution_count": 518,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'WORD'), ('Bob', 'WORD'), ('!', 'OTHER'), (\"It's\", 'WORD'), ('Mary', 'WORD'), (',', 'OTHER'), ('your', 'WORD'), ('mother-in-law', 'WORD'), (',', 'OTHER'), ('the', 'WORD'), ('mistake', 'WORD'), ('is', 'WORD'), ('your', 'WORD'), (\"parents'\", 'WORD'), ('!', 'OTHER'), ('-', 'OTHER'), ('-', 'OTHER'), ('M', 'OTHER'), ('om', 'WORD')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "J8YG_fyHgKjw",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "260e3cd54ed67e65d63fbde9343a8306",
          "grade": true,
          "grade_id": "tok-comp",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# comprehensiveness test (5 points)\n",
        "_, t = zip(*tok(pages[9][2]))\n",
        "assert 5 == len(set(t))"
      ],
      "execution_count": 519,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2hBIAQRlCZB"
      },
      "source": [
        "# 3. Collect and merge page views\n",
        "\n",
        "Data about page views on Wikipedia is available at https://dumps.wikimedia.org and there is documentation about the [definition of a page view](https://meta.wikimedia.org/wiki/Research:Page_view) and the [format of lines](https://dumps.wikimedia.org/other/pagecounts-ez/) in the file. In the class project, you will need to use page view data that we'll provide for ALL of English Wikipedia from the month of August 2021, which is more than 10.7 million viewed articles. The commented out code below shows how we generate that data, no need to run it yourself, this is just for your information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hv947ULyiT5"
      },
      "source": [
        "# # Paths\n",
        "## Using user page views (as opposed to spiders and automated traffic) for the \n",
        "##month of August 2021\n",
        "#pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
        "#p = Path(pv_path) \n",
        "#pv_name = p.name\n",
        "#pv_temp = f'{p.stem}-4dedup.txt'\n",
        "#pv_clean = f'{p.stem}.pkl'\n",
        "#Download the file (2.3GB) \n",
        "#!wget -N $pv_path\n",
        " #Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
        " #total number of page views (5). Then, remove lines with article id or page \n",
        " #view values that are not a sequence of digits.\n",
        "#!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
        " #Create a Counter (dictionary) that sums up the pages views for the same \n",
        " #article, resulting in a mapping from article id to total page views.\n",
        "#wid2pv = Counter()\n",
        "#with open(pv_temp, 'rt') as f:\n",
        " # for line in f:\n",
        "  #  parts = line.split(' ')\n",
        "   # wid2pv.update({int(parts[0]): int(parts[1])})\n",
        "  #write out the counter as binary file (pickle it)\n",
        "#with open(pv_clean, 'wb') as f:\n",
        " #  pickle.dump(wid2pv, f)\n",
        "# read in the counter\n",
        "#with open(pv_clean, 'rb') as f:\n",
        " #  wid2pv = pickle.loads(f.read())"
      ],
      "execution_count": 520,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baQSJ9-CbKFR"
      },
      "source": [
        "In order to keep things simple, in this assignment we provide you with a small sample of articles and their page view counts.\n",
        "\n",
        "**YOUR TASK (5 POINTS)**: Complete the implementation of `most_viewed` for ranking articles from the most viewed to the least viewed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "fvW_GMjYcI4u",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0cd167f1f4610328560383f81a8137c2",
          "grade": false,
          "grade_id": "page-view-ans",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# A counter mapping article id to number of page views\n",
        "wid2pv = Counter({\n",
        "    '17324616': 10, '17324662': 4, '17324672': 16, '17324677': 612, \n",
        "    '17324689': 66, '17324702': 274, '17324704': 49, '17324943': 76, \n",
        "    '17324721': 35, '17324736': 2801, '17324747': 641, '17324758': 33,\n",
        "    '17324768': 26, '17324783': 28, '17324788': 575, '17324790': 43, \n",
        "    '17324802': 29, '17324816': 159, '17324818': 57, '17324823': 60,\n",
        "    '17324834': 19, '17324835': 7, '17324893': 116, '17324908': 2038,\n",
        "    '15580374': 181126232, '1610886': 4657885, '30635': 8143874, \n",
        "    '3390': 4525604, '49632909': 5027640, '51150040': 3284643, \n",
        "    '60827': 4323859, '623737': 3427102, '65984422': 3733064, '737': 6039676\n",
        "})\n",
        "def myfunc(tup):\n",
        "  return tup[1]\n",
        "  \n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "def most_viewed(pages):\n",
        "  \"\"\"Rank pages from most viewed to least viewed using the above `wid2pv` \n",
        "     counter.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    pages: An iterable list of pages as returned from `page_iter` where each \n",
        "           item is an article with (id, title, body)\n",
        "  Returns:\n",
        "  --------\n",
        "  A list of tuples\n",
        "    Sorted list of articles from most viewed to least viewed article with \n",
        "    article title and page views. For example:\n",
        "    [('Langnes, Troms': 16), ('Langenes': 10), ('Langenes, Finnmark': 4), ...]\n",
        "  \"\"\"\n",
        " \n",
        "  lst=[]\n",
        "  for page in pages:\n",
        "    tup=(page[1],wid2pv[page[0]])\n",
        "    lst.append(tup)\n",
        "  lst.sort(key=myfunc,reverse=True)\n",
        "  return lst\n",
        " \n",
        " "
      ],
      "execution_count": 521,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "R7HwO9KugJOm",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c210d29457d4684081c22d0423fe2c2f",
          "grade": true,
          "grade_id": "page-views-test",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "pages_ranked = most_viewed(pages)\n",
        "_, views = zip(*pages_ranked)\n",
        "assert 25 == len(pages_ranked)\n",
        "assert ('No W', 274) in pages_ranked\n",
        "assert 7774 == sum(views)\n",
        "\n",
        "assert 2226 == views[0] - views[4]\n",
        "assert 10 == round(sum(views[0:5]) / sum(views[5:10]))"
      ],
      "execution_count": 522,
      "outputs": []
    }
  ]
}